# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Capsule autoencoder implementation."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sonnet as snt
import tensorflow as tf
import tensorflow_probability as tfp

from capsules import capsule as _capsule
from capsules import math_ops
from capsules import probe
from capsules.models import Model

tfd = tfp.distributions


class ImageCapsule(snt.AbstractModule):
  """Capsule decoder for constellations."""

  def __init__(self, n_caps, n_caps_dims, n_votes, **capsule_kwargs):
    """Builds the module.

    Args:
      n_caps: int, number of capsules.
      n_caps_dims: int, number of capsule coordinates.
      n_votes: int, number of votes generated by each capsule.
      **capsule_kwargs: kwargs passed to capsule layer.
    """
    super(ImageCapsule, self).__init__()
    self._n_caps = n_caps
    self._n_caps_dims = n_caps_dims
    self._n_votes = n_votes
    self._capsule_kwargs = capsule_kwargs

  def _build(self, h, x, presence=None):
    """Builds the module.

    Args:
      h: Tensor of encodings of shape [B, n_enc_dims].
      x: Tensor of inputs of shape [B, n_points, n_input_dims]
      presence: Tensor of shape [B, n_points, 1] or None; if it exists, it
        indicates which input points exist.

    Returns:
      A bunch of stuff.
    """
    batch_size = int(x.shape[0])

    capsule = _capsule.CapsuleLayer(self._n_caps, self._n_caps_dims,
                                    self._n_votes, **self._capsule_kwargs)

    res = capsule(h)
    vote_shape = [batch_size, self._n_caps, self._n_votes, 6]
    res.vote = tf.reshape(res.vote[Ellipsis, :-1, :], vote_shape)

    votes, scale, vote_presence_prob = res.vote, res.scale, res.vote_presence

    likelihood = _capsule.CapsuleLikelihood(votes, scale, vote_presence_prob)
    ll_res = likelihood(x, presence)
    res.update(ll_res._asdict())

    caps_presence_prob = tf.reduce_max(
        tf.reshape(vote_presence_prob,
                   [batch_size, self._n_caps, self._n_votes]), 2)

    res.caps_presence_prob = caps_presence_prob
    return res


class ImageAutoencoder(Model):
  """Capsule autoencoder."""

  def __init__(
      self,
      primary_encoder,
      primary_decoder,
      encoder,
      decoder,
      input_key,
      label_key=None,
      n_classes=None,
      dynamic_l2_weight=0.,
      caps_ll_weight=0.,
      vote_type='soft',
      pres_type='enc',
      img_summaries=False,
      stop_grad_caps_inpt=False,
      stop_grad_caps_target=False,
      prior_sparsity_loss_type='kl',
      prior_within_example_sparsity_weight=0.,
      prior_between_example_sparsity_weight=0.,
      prior_within_example_constant=0.,
      posterior_sparsity_loss_type='kl',
      posterior_within_example_sparsity_weight=0.,
      posterior_between_example_sparsity_weight=0.,
      primary_caps_sparsity_weight=0.,
      weight_decay=0.,
      feed_templates=True,
      prep='none',
  ):

    super(ImageAutoencoder, self).__init__()
    self._primary_encoder = primary_encoder
    self._primary_decoder = primary_decoder
    self._encoder = encoder
    self._decoder = decoder
    self._input_key = input_key
    self._label_key = label_key
    self._n_classes = n_classes

    self._dynamic_l2_weight = dynamic_l2_weight
    self._caps_ll_weight = caps_ll_weight
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._img_summaries = img_summaries

    self._stop_grad_caps_inpt = stop_grad_caps_inpt
    self._stop_grad_caps_target = stop_grad_caps_target
    self._prior_sparsity_loss_type = prior_sparsity_loss_type
    self._prior_within_example_sparsity_weight = prior_within_example_sparsity_weight
    self._prior_between_example_sparsity_weight = prior_between_example_sparsity_weight
    self._prior_within_example_constant = prior_within_example_constant
    self._posterior_sparsity_loss_type = posterior_sparsity_loss_type
    self._posterior_within_example_sparsity_weight = posterior_within_example_sparsity_weight
    self._posterior_between_example_sparsity_weight = posterior_between_example_sparsity_weight
    self._primary_caps_sparsity_weight = primary_caps_sparsity_weight
    self._weight_decay = weight_decay
    self._feed_templates = feed_templates

    self._prep = prep

  @staticmethod
  def normalized_sobel_edges(img,
                             subtract_median=True,
                             same_number_of_channels=True):
    """Applies the sobel filter to images and normalizes the result.

    Args:
      img: tensor of shape [B, H, W, C].
      subtract_median: bool; if True it subtracts the median from every channel.
        This makes constant backgrounds black.
      same_number_of_channels: bool; returnd tensor has the same number of
        channels as the input tensor if True.

    Returns:
      Tensor of shape [B, H, W, C] if same_number_of_channels
      else [B, H, W, 2C].
    """

    sobel_img = tf.image.sobel_edges(img)

    if same_number_of_channels:
      sobel_img = tf.reduce_sum(sobel_img, -1)
    else:
      n_channels = int(img.shape[-1])
      sobel_img = tf.reshape(sobel_img,
                             sobel_img.shape[:-2].concatenate(2 * n_channels))

    if subtract_median:
      sobel_img = abs(sobel_img - tf.contrib.distributions.percentile(
        sobel_img, 50.0, axis=(1, 2), keep_dims=True))

    smax = tf.reduce_max(sobel_img, (1, 2), keepdims=True)
    smin = tf.reduce_min(sobel_img, (1, 2), keepdims=True)
    sobel_img = (sobel_img - smin) / (smax - smin + 1e-8)
    return sobel_img

  def _img(self, data, prep='none'):

    img = data[self._input_key]
    if prep == 'sobel':
      img = ImageAutoencoder.normalized_sobel_edges(img)

    return img

  def _label(self, data):
    return data.get(self._label_key, None)

  def _build(self, data):

    input_x = self._img(data, False)
    target_x = self._img(data, prep=self._prep)
    batch_size = int(input_x.shape[0])

    primary_caps = self._primary_encoder(input_x)
    pres = primary_caps.presence

    expanded_pres = tf.expand_dims(pres, -1)
    pose = primary_caps.pose
    input_pose = tf.concat([pose, 1. - expanded_pres], -1)

    input_pres = pres
    if self._stop_grad_caps_inpt:
      input_pose = tf.stop_gradient(input_pose)
      input_pres = tf.stop_gradient(pres)

    target_pose, target_pres = pose, pres
    if self._stop_grad_caps_target:
      target_pose = tf.stop_gradient(target_pose)
      target_pres = tf.stop_gradient(target_pres)

    # skip connection from the img to the higher level capsule
    if primary_caps.feature is not None:
      input_pose = tf.concat([input_pose, primary_caps.feature], -1)

    # try to feed presence as a separate input
    # and if that works, concatenate templates to poses
    # this is necessary for set transformer
    n_templates = int(primary_caps.pose.shape[1])
    templates = self._primary_decoder.make_templates(n_templates,
                                                     primary_caps.feature)

    try:
      if self._feed_templates:
        inpt_templates = templates
        if self._stop_grad_caps_inpt:
          inpt_templates = tf.stop_gradient(inpt_templates)

        if inpt_templates.shape[0] == 1:
          inpt_templates = snt.TileByDim([0], [batch_size])(inpt_templates)
        inpt_templates = snt.BatchFlatten(2)(inpt_templates)
        pose_with_templates = tf.concat([input_pose, inpt_templates], -1)
      else:
        pose_with_templates = input_pose

      h = self._encoder(pose_with_templates, input_pres)

    except TypeError:
      h = self._encoder(input_pose)

    res = self._decoder(h, target_pose, target_pres)
    res.primary_presence = primary_caps.presence

    if self._vote_type == 'enc':
      primary_dec_vote = primary_caps.pose
    elif self._vote_type == 'soft':
      primary_dec_vote = res.soft_winner
    elif self._vote_type == 'hard':
      primary_dec_vote = res.winner
    else:
      raise ValueError('Invalid vote_type="{}"".'.format(self._vote_type))

    if self._pres_type == 'enc':
      primary_dec_pres = pres
    elif self._pres_type == 'soft':
      primary_dec_pres = res.soft_winner_pres
    elif self._pres_type == 'hard':
      primary_dec_pres = res.winner_pres
    else:
      raise ValueError('Invalid pres_type="{}"".'.format(self._pres_type))

    res.bottom_up_rec = self._primary_decoder(
        primary_caps.pose,
        primary_caps.presence,
        template_feature=primary_caps.feature,
        img_embedding=primary_caps.img_embedding)

    res.top_down_rec = self._primary_decoder(
        res.winner,
        primary_caps.presence,
        template_feature=primary_caps.feature,
        img_embedding=primary_caps.img_embedding)

    rec = self._primary_decoder(
        primary_dec_vote,
        primary_dec_pres,
        template_feature=primary_caps.feature,
        img_embedding=primary_caps.img_embedding)

    tile = snt.TileByDim([0], [res.vote.shape[1]])
    tiled_presence = tile(primary_caps.presence)

    tiled_feature = primary_caps.feature
    if tiled_feature is not None:
      tiled_feature = tile(tiled_feature)

    tiled_img_embedding = tile(primary_caps.img_embedding)

    res.top_down_per_caps_rec = self._primary_decoder(
        snt.MergeDims(0, 2)(res.vote),
        snt.MergeDims(0, 2)(res.vote_presence) * tiled_presence,
        template_feature=tiled_feature,
        img_embedding=tiled_img_embedding)

    res.templates = templates
    res.template_pres = pres
    res.used_templates = rec.transformed_templates

    res.rec_mode = rec.pdf.mode()
    res.rec_mean = rec.pdf.mean()

    res.mse_per_pixel = tf.square(target_x - res.rec_mode)
    res.mse = math_ops.flat_reduce(res.mse_per_pixel)

    res.rec_ll_per_pixel = rec.pdf.log_prob(target_x)
    res.rec_ll = math_ops.flat_reduce(res.rec_ll_per_pixel)

    n_points = int(res.posterior_mixing_probs.shape[1])
    mass_explained_by_capsule = tf.reduce_sum(res.posterior_mixing_probs, 1)

    (res.posterior_within_sparsity_loss,
     res.posterior_between_sparsity_loss) = _capsule.sparsity_loss(
         self._posterior_sparsity_loss_type,
         mass_explained_by_capsule / n_points,
         num_classes=self._n_classes)

    (res.prior_within_sparsity_loss,
     res.prior_between_sparsity_loss) = _capsule.sparsity_loss(
         self._prior_sparsity_loss_type,
         res.caps_presence_prob,
         num_classes=self._n_classes,
         within_example_constant=self._prior_within_example_constant)

    # EDITED
    label = self._label(data)
    (res.posterior_cls_xe,
     res.posterior_cls_acc,
     res.posterior_cls_pred,
     res.posterior_cls_logits) = probe.classification_probe(
      mass_explained_by_capsule,
      label,
      self._n_classes,
      labeled=data.get('labeled', None))
    (res.prior_cls_xe,
     res.prior_cls_acc,
     res.prior_cls_pred,
     res.prior_cls_logits) = probe.classification_probe(
      res.caps_presence_prob,
      label,
      self._n_classes,
      labeled=data.get('labeled', None))

    # EDITED
    res.best_cls_acc = None if label is None else tf.maximum(res.prior_cls_acc, res.posterior_cls_acc)

    res.primary_caps_l1 = math_ops.flat_reduce(res.primary_presence)

    if self._weight_decay > 0.0:
      decay_losses_list = []
      for var in tf.trainable_variables():
        if 'w:' in var.name or 'weights:' in var.name:
          decay_losses_list.append(tf.nn.l2_loss(var))
      res.weight_decay_loss = tf.reduce_sum(decay_losses_list)
    else:
      res.weight_decay_loss = 0.0

    return res

  def _loss(self, data, res):

    loss = (-res.rec_ll - self._caps_ll_weight * res.log_prob +
            self._dynamic_l2_weight * res.dynamic_weights_l2 +
            self._primary_caps_sparsity_weight * res.primary_caps_l1 +
            self._posterior_within_example_sparsity_weight *
            res.posterior_within_sparsity_loss -
            self._posterior_between_example_sparsity_weight *
            res.posterior_between_sparsity_loss +
            self._prior_within_example_sparsity_weight *
            res.prior_within_sparsity_loss -
            self._prior_between_example_sparsity_weight *
            res.prior_between_sparsity_loss +
            self._weight_decay * res.weight_decay_loss
           )

    try:
      loss += res.posterior_cls_xe + res.prior_cls_xe
    # EDITED
    except (AttributeError, TypeError):
      pass

    return loss
